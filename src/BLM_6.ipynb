{
 "cells": [
  {
   "source": [
    "# BLM Activity and Sentiment Analysis\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Divide each major period into 3 sub-periods using change point analysis. This was performed in `tweet_counts.ipynb`.\n",
    "2. For each period...\n",
    "   * Process all tweets \n",
    "      * Process each tweet through `TweetsManager.process_tweet()`\n",
    "      * Call `TweetsManager.process_deferred_interactions()` to handle retweet/reply corner cases.\n",
    "      * Call `TweetsManager.analyze_graph()` to detect communities.\n",
    "   * Discard too-small communities.\n",
    "   * Assign BLM stance to communities.\n",
    "      * Generate reports on largest communities.\n",
    "      * Manually assign stance to those communities.\n",
    "      * Build/use prediction model for remaining communities.\n",
    "   * Save everything."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from   collections import Counter, defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "from   os import path\n",
    "import pandas as pd\n",
    "import re\n",
    "from   string import Template\n",
    "from   typing import List\n",
    "\n",
    "from elasticsearch import Elasticsearch as ES\n",
    "from elasticsearch.helpers import scan\n",
    "\n",
    "from blm_activity_db import BlmActivityDb\n",
    "from community_classifier import get_blm_classifier, get_three_class_classifier\n",
    "from community_report import generate_init_community_report\n",
    "from tweet_mgr import TweetsManager, CommunityActivity, Stance\n",
    "from tweet_sentiment import EmoScores, PronounCounts, SentimentAnalysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurable parameters\n",
    "start_date = \"2020-06-14\"\n",
    "end_date = \"2020-10-31\"\n",
    "es_idx = 'tweets2'\n",
    "# Template for query_body\n",
    "#query_body = {\n",
    "#  \"query\": {\n",
    "#      \"range\": {\n",
    "#          \"doc.created_at\": {\n",
    "#            \"gte\": \"Sun Nov 23 00:00:00 +0000 2014\",\n",
    "#            \"lt\": \"Wed Dec 24 00:00:00 +0000 2014\"\n",
    "#          }\n",
    "#      }\n",
    "#  }\n",
    "#}\n",
    "\n",
    "query_body = {\n",
    "  \"query\": {\n",
    "      \"range\": {\n",
    "          \"doc.created_at\": {\n",
    "              \"gte\": \"Sun Jun 14 00:00:00 +0000 2020\",\n",
    "              \"lt\": \"Sun Nov 01 00:00:00 +0000 2020\"\n",
    "          }\n",
    "      }\n",
    "  }\n",
    "}\n",
    "\n",
    "period = 6\n",
    "num_init_communities = 40\n",
    "num_exemplars = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tm = TweetsManager()\n",
    "\n",
    "es = ES(hosts=[\"localhost\"])\n",
    "scan_iter = scan(es, index=es_idx, query=query_body)\n",
    "for result in scan_iter:\n",
    "    tweet = result['_source']\n",
    "    tm.process_tweet(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_text_by_id(id_):\n",
    "    doc = es.get(index=es_idx, id=id_)\n",
    "    return doc['_source']['doc']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm.process_deferred_interactions()\n",
    "tm.analyze_graph(n_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_size = Counter() # num_accounts -> num of communities having that size \n",
    "unique_tweets = Counter() # num_unique_tweets -> num of communities with that activity\n",
    "for _, accounts in tm.community_user_map.items():\n",
    "    community_size[len(accounts)] += 1\n",
    "for c_activity in tm.community_activity_map.values():\n",
    "    num_unique_tweets = c_activity.num_tweets - len(c_activity.retweet_sentiment_analyses)\n",
    "    unique_tweets[num_unique_tweets] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "75642 :  75642\n56276 :  56276\n9003 :  9003\n51336 :  51336\n23918 :  23918\n11560 :  11560\n5214 :  5214\n1674 :  1674\n2732 :  2732\n1418 :  1418\n1354 :  1354\n1038 :  1038\n1698 :  1698\n3981 :  3981\n1525 :  1525\n2535 :  2535\n3644 :  3644\n1727 :  1727\n595 :  595\n3888 :  3888\n108 :  216\n1668 :  1668\n997 :  997\n2085 :  2085\n1020 :  1020\n2393 :  2393\n380 :  380\n916 :  916\n1835 :  1835\n2408 :  2408\n3282 :  3282\n1759 :  1759\n5409 :  5409\n683 :  683\n1481 :  1481\n2076 :  2076\n2694 :  2694\n2224 :  2224\n3390 :  3390\n1131 :  1131\n1673 :  1673\n1326 :  1326\n3007 :  3007\n751 :  751\n2998 :  2998\n75 :  75\n678 :  678\n2044 :  2044\n985 :  985\n831 :  831\n1461 :  1461\n866 :  866\n1715 :  1715\n2182 :  2182\n2950 :  2950\n1986 :  1986\n833 :  833\n2165 :  2165\n576 :  576\n1889 :  1889\n153 :  153\n1220 :  2440\n742 :  742\n1316 :  1316\n1214 :  2428\n1064 :  1064\n788 :  788\n3669 :  3669\n583 :  583\n1795 :  1795\n734 :  734\n898 :  898\n1158 :  1158\n1008 :  1008\n103 :  103\n165 :  165\n837 :  837\n1684 :  1684\n609 :  609\n944 :  944\n1433 :  1433\n568 :  568\n2210 :  2210\n784 :  784\n478 :  478\n1092 :  1092\n794 :  794\n1442 :  1442\n1110 :  1110\n919 :  919\n253 :  253\n1887 :  1887\n125 :  125\n390 :  390\n1780 :  1780\n287 :  287\n348 :  348\n376 :  376\n426 :  426\n523 :  523\n467 :  467\n3699 :  3699\n927 :  927\n3876 :  3876\n984 :  984\n653 :  653\n477 :  477\n764 :  764\n1570 :  1570\n2548 :  2548\n946 :  946\n884 :  884\n1647 :  1647\n58 :  116\n619 :  619\n457 :  457\n268 :  268\n313 :  313\n205 :  410\n873 :  873\n1337 :  1337\n90 :  270\n257 :  257\n879 :  879\n497 :  497\n189 :  189\n93 :  93\n37 :  185\n55 :  165\n38 :  342\n30 :  300\n180 :  180\n21 :  546\n274 :  274\n433 :  433\n3 :  8559\n83 :  83\n87 :  174\n1 :  24148\n27 :  189\n2 :  13414\n13 :  1001\n25 :  350\n14 :  1078\n98 :  98\n69 :  69\n8 :  2312\n4 :  5848\n20 :  340\n15 :  960\n29 :  348\n9 :  1863\n6 :  3648\n35 :  210\n12 :  1512\n18 :  522\n39 :  117\n11 :  1221\n7 :  2821\n10 :  1620\n33 :  198\n17 :  850\n5 :  4690\n67 :  67\n22 :  352\n65 :  130\n34 :  272\n47 :  188\n36 :  252\n16 :  848\n40 :  80\n26 :  312\n19 :  551\n78 :  78\n32 :  160\n28 :  252\n23 :  414\n48 :  48\n51 :  51\n41 :  246\n61 :  61\n134 :  134\n44 :  176\n31 :  217\n81 :  81\n24 :  264\n49 :  196\n66 :  132\n53 :  159\n129 :  129\n228 :  228\n112 :  112\n52 :  156\n80 :  80\n68 :  204\n77 :  231\n455 :  455\n43 :  86\n74 :  148\n45 :  45\n57 :  57\n59 :  59\n127 :  127\n50 :  100\n140 :  140\n91 :  91\n60 :  60\n146 :  146\n258 :  258\n374 :  374\n170 :  170\n54 :  54\n76 :  152\n115 :  115\n188 :  188\n529 :  529\n63 :  63\n96 :  96\n"
     ]
    }
   ],
   "source": [
    "total_unique_tweets = {k: k*v for k, v in unique_tweets.items()}\n",
    "for k, v in total_unique_tweets.items():\n",
    "    print(k, \": \", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total tweets: 1758603\ntotal unique tweets: 487201\n708918 accounts in 38819 communities\nIf 20 used as threshold for unique tweets per community...\n426 communities and 409735 unique tweets remain.\n"
     ]
    }
   ],
   "source": [
    "print(\"total tweets:\", len(tm.tweets))\n",
    "print(\"total unique tweets:\", sum(v for v in total_unique_tweets.values()))\n",
    "print(len(tm.user_community_map), \"accounts in\", len(tm.community_user_map), \"communities\")\n",
    "print(\"If 20 used as threshold for unique tweets per community...\")\n",
    "num_communities = sum(v for k, v in unique_tweets.items() if k >= 20)\n",
    "num_unique_tweets = sum(v for k, v in total_unique_tweets.items() if k >= 20)\n",
    "print(num_communities, \"communities and\", num_unique_tweets, \"unique tweets remain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm.filter_low_activity_communities(unique_tweets_threshold=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Initial Report\n",
    "report_dir = f'../data/Reports/{period}/'\n",
    "report_name = \"Largest Communities Hashtags and Tweets\"\n",
    "report_path = report_dir + f\"{report_name}.md\"\n",
    "\n",
    "report = f\"# {report_name} in Period {period}\\n\\n\"\n",
    "\n",
    "# Add section for each of top num_init_communities by membership count\n",
    "comm_user_counts = sorted(tm.community_user_map.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "# derive inter-community replies, retweets\n",
    "top_community_ids = set(x[0] for i, x in enumerate(comm_user_counts) if i < num_init_communities)\n",
    "reply_counter = Counter()\n",
    "replied_to_counter = Counter()\n",
    "retweeted_counter = Counter()\n",
    "for (_, comm_reply), count in tm.inter_comm_reply_counter.items():\n",
    "    if comm_reply.replying in top_community_ids:\n",
    "        reply_counter[comm_reply.replying] += count\n",
    "    if comm_reply.replied_to in top_community_ids:\n",
    "        replied_to_counter[comm_reply.replied_to] += count\n",
    "for (_, comm_retweet), count in tm.inter_comm_retweet_counter.items():\n",
    "    if comm_retweet.retweeted in top_community_ids:\n",
    "        retweeted_counter[comm_retweet.retweeted] += count\n",
    "\n",
    "# extract other community metrics, create report section\n",
    "for k, (comm_id, members) in enumerate(comm_user_counts):\n",
    "    if k == num_init_communities:\n",
    "        break\n",
    "    num_members = len(members)\n",
    "    ca: CommunityActivity = tm.community_activity_map[comm_id]\n",
    "    num_tweets = ca.num_tweets\n",
    "    num_retweets = sum(count for count in ca.retweet_counter.values())\n",
    "    # influence ranks\n",
    "    ranks = []\n",
    "    for member in members:\n",
    "        ranks.append(tm.user_activity[member].influence_rank)\n",
    "    top10_influence_ranks = sorted(ranks)[:10]\n",
    "    # memes\n",
    "    hashtags = []\n",
    "    ht_counts = []\n",
    "    meme_counts = sorted(ca.meme_counter.items(), key=lambda x:x[1], reverse=True)\n",
    "    for i, (tag, count) in enumerate(meme_counts):\n",
    "        if i == num_exemplars:\n",
    "            break\n",
    "        hashtags.append(tag)\n",
    "        ht_counts.append(count)\n",
    "    # retweets\n",
    "    tweet_ids = []\n",
    "    rt_counts = []\n",
    "    retweet_counts = sorted(ca.retweet_counter.items(), key=lambda x:x[1], reverse=True)\n",
    "    for i, (tweet_id, count) in enumerate(retweet_counts):\n",
    "        if i == num_exemplars:\n",
    "            break\n",
    "        tweet_ids.append(tweet_id)\n",
    "        rt_counts.append(count)\n",
    "    rts = []\n",
    "    for id_ in tweet_ids:\n",
    "        if id_ in tm.tweets:\n",
    "            rts.append(tm.tweets[id_])\n",
    "        else:\n",
    "            rts.append(get_tweet_text_by_id(id_))\n",
    "    report += generate_init_community_report(\n",
    "        comm_id,\n",
    "        num_members,\n",
    "        num_tweets,\n",
    "        num_retweets,\n",
    "        hashtags, \n",
    "        ht_counts, \n",
    "        rts, \n",
    "        rt_counts,\n",
    "        retweeted_counter[comm_id],\n",
    "        reply_counter[comm_id],\n",
    "        replied_to_counter[comm_id],\n",
    "        top10_influence_ranks,\n",
    ")\n",
    "with open(report_path, 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_comm_ids = [0, 4, 12, 16,  21, 32, 38]\n",
    "excluded_comm_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blm_comm_ids = [i for i in range(num_init_communities) if not i in counter_comm_ids]\n",
    "blm_comm_ids = [x[0] for i, x in enumerate(comm_user_counts) \n",
    "                if i < num_init_communities and\n",
    "                x[0] not in counter_comm_ids and\n",
    "                x[0] not in excluded_comm_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def get_tweet_texts(tweet_ids, tm):\n",
    "    texts = []\n",
    "    for id_ in tweet_ids:\n",
    "        if id_ in tm.tweets:\n",
    "            texts.append(tm.tweets[id_])\n",
    "        else:\n",
    "            texts.append(get_tweet_text_by_id(id_))\n",
    "    return texts\n",
    "\n",
    "blm_retweet_set = set()\n",
    "counter_retweet_set = set()\n",
    "excluded_retweet_set = set()\n",
    "for k, (comm_id, _ ) in enumerate(comm_user_counts):\n",
    "    if k == num_init_communities:\n",
    "        break\n",
    "    num_exemplars = 288 if comm_id not in blm_comm_ids else 18\n",
    "    retweet_counts = sorted(\n",
    "        tm.community_activity_map[comm_id].retweet_counter.items(), \n",
    "        key=lambda x:x[1], \n",
    "        reverse=True\n",
    "    )\n",
    "    for i, (tweet_id, _ ) in enumerate(retweet_counts):\n",
    "        if i == num_exemplars:\n",
    "            break\n",
    "        if comm_id in counter_comm_ids:\n",
    "            counter_retweet_set.add(tweet_id)\n",
    "        elif comm_id in excluded_comm_ids:\n",
    "            excluded_retweet_set.add(tweet_id)\n",
    "        else:\n",
    "            blm_retweet_set.add(tweet_id)\n",
    "blm_retweets = get_tweet_texts(blm_retweet_set, tm)\n",
    "counter_retweets = get_tweet_texts(counter_retweet_set, tm)\n",
    "excluded_retweets = get_tweet_texts(excluded_retweet_set, tm)\n",
    "blm_clf, cv_results = get_blm_classifier(blm_retweets, counter_retweets)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0       5.466185      0.182284         1.340854        0.084047   \n1       5.400599      0.044618         1.351356        0.032278   \n2       5.524687      0.099842         1.386248        0.053888   \n\n  param_model__alpha param_model__fit_prior  \\\n0                  1                  False   \n1                  1                  False   \n2                  1                  False   \n\n                               param_vec__stop_words  \\\n0          [rt, #blacklivesmatter, blacklivesmatter]   \n1  [rt, blacklivesmatter, #blacklivesmatter, i, m...   \n2  [i, me, my, myself, we, our, ours, ourselves, ...   \n\n                                              params  split0_test_score  \\\n0  {'model__alpha': 1.0, 'model__fit_prior': Fals...           0.894636   \n1  {'model__alpha': 1.0, 'model__fit_prior': Fals...           0.896552   \n2  {'model__alpha': 1.0, 'model__fit_prior': Fals...           0.894636   \n\n   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n0           0.877395           0.875479           0.886973           0.885057   \n1           0.885057           0.865900           0.892720           0.886973   \n2           0.886973           0.867816           0.892720           0.886973   \n\n   mean_test_score  std_test_score  rank_test_score  \n0         0.883908        0.006918                3  \n1         0.885441        0.010590                2  \n2         0.885824        0.009509                1  \n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(cv_results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per_comm = 20\n",
    "for k, (comm_id, _ ) in enumerate(comm_user_counts):\n",
    "    if k < num_init_communities:\n",
    "        continue\n",
    "    sample_tweet_ids = []\n",
    "    counts = []\n",
    "    retweet_counts = sorted(\n",
    "        tm.community_activity_map[comm_id].retweet_counter.items(), \n",
    "        key=lambda x:x[1], \n",
    "        reverse=True\n",
    "    )\n",
    "    for i, (tweet_id, count) in enumerate(retweet_counts):\n",
    "        if i == num_samples_per_comm:\n",
    "            break\n",
    "        sample_tweet_ids.append(tweet_id)\n",
    "        counts.append(count)\n",
    "    if sum(counts) < num_samples_per_comm:\n",
    "        excluded_comm_ids.append(comm_id)\n",
    "        continue\n",
    "    sample_tweets = get_tweet_texts(sample_tweet_ids, tm)\n",
    "    stance_predictions = blm_clf.predict(sample_tweets)\n",
    "    weighted_sum = np.dot(np.array(counts), np.array(stance_predictions))\n",
    "    stance_probability = weighted_sum / sum(counts)\n",
    "    if stance_probability < -0.3:\n",
    "        counter_comm_ids.append(comm_id)\n",
    "    elif stance_probability > 0.3:\n",
    "        blm_comm_ids.append(comm_id)\n",
    "    else:\n",
    "        excluded_comm_ids.append(comm_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "counter communities: [0, 4, 12, 16, 21, 32, 38]\n294 unknown communities\n125 BLM communities\n"
     ]
    }
   ],
   "source": [
    "print(f\"counter communities: {counter_comm_ids}\")\n",
    "print(f\"{len(excluded_comm_ids)} unknown communities\")\n",
    "print(f\"{len(blm_comm_ids)} BLM communities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_ in counter_comm_ids:\n",
    "    tm.community_activity_map[id_].stance = Stance.CounterProtest\n",
    "for id_ in excluded_comm_ids:\n",
    "    tm.community_activity_map[id_].stance = Stance.Unknown\n",
    "for id_ in blm_comm_ids:\n",
    "    tm.community_activity_map[id_].stance = Stance.Protest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = BlmActivityDb() \n",
    "db.save_tweets_mgr(tm, period)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 25 # i.e, number of tweets or memes to display\n",
    "movement_template = Template('''\n",
    "## MOVEMENT $movement\n",
    "\n",
    "Communities: $num_communities  \n",
    "Members: $num_members  \n",
    "Retweets: $num_retweets  \n",
    "Tweets: $num_tweets\n",
    "\n",
    "### Top Hashtags\n",
    "\n",
    "| Count | Hashtag |\n",
    "|------:|:------|\n",
    "$hashtag_list\n",
    "\n",
    "### Top Retweets\n",
    "\n",
    "| Count | Tweet |\n",
    "|------:|:------|\n",
    "$retweet_list\n",
    "\n",
    "### Sentiment\n",
    "\n",
    "All Tweet Polarity = $at_polarity  \n",
    "Retweet Polarity = $rt_polarity\n",
    "\n",
    "### Emotions\n",
    "\n",
    "| Emotion | All Tweets | Retweets |\n",
    "|------:|:------:|:-------|\n",
    "$emo_list\n",
    "\n",
    "### Pronoun Usage\n",
    "\n",
    "| Person | All Tweets | Retweets |\n",
    "|------:|:------:|:-------|\n",
    "$pronoun_columns\n",
    "\n",
    "''')\n",
    "\n",
    "def printable_top_hashtag_list(meme_counter):\n",
    "    top_memes = sorted(meme_counter.items(), key = lambda x: x[1], reverse = True)\n",
    "    hashtag_list = \"\"\n",
    "    for i, (ht, count) in enumerate(top_memes):\n",
    "        if i == num_examples:\n",
    "            break\n",
    "        hashtag_list += f\"| {count} | {ht} |\\n\"\n",
    "    return hashtag_list\n",
    "\n",
    "\n",
    "line_feeds = re.compile(\"[\\r\\n]\")\n",
    "def printable_top_retweet_list(retweet_counter):\n",
    "    top_retweets = sorted(retweet_counter.items(), key = lambda x: x[1], reverse = True)\n",
    "    retweet_list = \"\"\n",
    "    for i, (tweet_id, count) in enumerate(top_retweets):\n",
    "        if i == num_examples:\n",
    "            break\n",
    "        tweet = get_tweet_text_by_id(tweet_id)\n",
    "        tweet = line_feeds.sub('', tweet)\n",
    "        retweet_list += f\"| {count} | {tweet} |\\n\"\n",
    "    return retweet_list\n",
    "\n",
    "\n",
    "def printable_emo_scores_columns(left: EmoScores, right: EmoScores):\n",
    "    emo_list = \"\"\n",
    "    emo_list += f\"| trust | {round(left.trust, 3)} | {round(right.trust, 3)} |\\n\"\n",
    "    emo_list += f\"| anticipation | {round(left.anticipation, 3)} | {round(right.anticipation, 3)} |\\n\"\n",
    "    emo_list += f\"| joy | {round(left.joy, 3)} | {round(right.joy, 3)} |\\n\"\n",
    "    emo_list += f\"| surprise | {round(left.surprise, 3)} | {round(right.surprise, 3)} |\\n\"\n",
    "    emo_list += f\"| anger | {round(left.anger, 3)} | {round(right.anger, 3)} |\\n\"\n",
    "    emo_list += f\"| disgust | {round(left.disgust, 3)} | {round(right.disgust, 3)} |\\n\"\n",
    "    emo_list += f\"| fear | {round(left.fear, 3)} | {round(right.fear, 3)} |\\n\"\n",
    "    emo_list += f\"| sadness | {round(left.sadness, 3)} | {round(right.sadness, 3)} |\\n\" \n",
    "    return emo_list  \n",
    "\n",
    "\n",
    "def printable_pronoun_usage_columns(left: PronounCounts, right: PronounCounts):\n",
    "    printable_columns = \"\"\n",
    "    printable_columns += f\"| First Singular | {round(left.first_singular, 3)} | {round(right.first_singular, 3)} |\\n\"\n",
    "    printable_columns += f\"| First Plural | {round(left.first_plural, 3)} | {round(right.first_plural, 3)} |\\n\"\n",
    "    printable_columns += f\"| Second | {round(left.second,3)} | {round(right.second, 3)} |\\n\"\n",
    "    printable_columns += f\"| Third | {round(left.third, 3)} | {round(right.third, 3)} |\\n\"\n",
    "    return printable_columns  \n",
    "\n",
    "\n",
    "def store_movement_reports(movement, report_dir, comm_ids, tm):\n",
    "    '''Write files with salient data on BLM or counter movement during a period\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    movement : str\n",
    "        \"BLM\" or \"Counter\"\n",
    "    report_dir : str\n",
    "        FS directory where reports are to be written\n",
    "    comm_ids : list of int\n",
    "        IDs of communities in movement\n",
    "    tm : TweetManager instance\n",
    "    '''\n",
    "    # movement stats\n",
    "    ## counts\n",
    "    num_communities = len(comm_ids)\n",
    "    if num_communities == 0:\n",
    "        return\n",
    "    num_members = 0\n",
    "    total_tweets = 0\n",
    "    total_retweets = 0\n",
    "    meme_counter = Counter()\n",
    "    retweet_counter = Counter()\n",
    "    retweet_pc, retweet_emo, retweet_sentiment = PronounCounts(), EmoScores(), 0.0\n",
    "    all_tweet_pc, all_tweet_emo, all_tweet_sentiment = PronounCounts(), EmoScores(), 0.0\n",
    "    for community_id in comm_ids:\n",
    "        num_members += len(tm.community_user_map[community_id])\n",
    "        c_activity = tm.community_activity_map[community_id]\n",
    "        total_tweets += c_activity.num_tweets\n",
    "        for tweet_id, count in c_activity.retweet_counter.items():\n",
    "            total_retweets += count\n",
    "            retweet_counter[tweet_id] += count\n",
    "        for meme, count in c_activity.meme_counter.items():\n",
    "            meme_counter[meme] += count\n",
    "        rss = c_activity.retweet_sentiment_summary\n",
    "        num_retweets = len(c_activity.retweet_sentiment_analyses)\n",
    "        retweet_pc += rss.pronoun_counts * num_retweets\n",
    "        retweet_emo += rss.emo_scores * num_retweets\n",
    "        retweet_sentiment += rss.sentiment * num_retweets\n",
    "        atss = c_activity.all_sentiment_summary\n",
    "        all_tweet_pc += atss.pronoun_counts * c_activity.num_tweets\n",
    "        all_tweet_emo += atss.emo_scores * c_activity.num_tweets\n",
    "        all_tweet_sentiment += atss.sentiment * c_activity.num_tweets\n",
    "    \n",
    "    retweet_pc /= total_retweets\n",
    "    retweet_emo /= total_retweets\n",
    "    retweet_sentiment /= total_retweets\n",
    "    all_tweet_pc /= total_tweets\n",
    "    all_tweet_emo /= total_tweets\n",
    "    all_tweet_sentiment /= total_tweets\n",
    "\n",
    "    ## 25 most important hashtags             \n",
    "    hashtag_list = printable_top_hashtag_list(meme_counter)\n",
    "    \n",
    "    ## 25 most retweeted\n",
    "    retweet_list = printable_top_retweet_list(retweet_counter)\n",
    "    \n",
    "    ## emotions\n",
    "    emo_list = printable_emo_scores_columns(left=all_tweet_emo, right=retweet_emo)\n",
    "    \n",
    "    ## Write to file\n",
    "    subs = {\n",
    "        \"movement\": movement,\n",
    "        \"num_communities\": num_communities,\n",
    "        \"num_members\": num_members,\n",
    "        \"num_tweets\": total_tweets,\n",
    "        \"num_retweets\": total_retweets,\n",
    "        \"hashtag_list\": hashtag_list,\n",
    "        \"retweet_list\": retweet_list,\n",
    "        \"at_polarity\": round(all_tweet_sentiment, 3),\n",
    "        \"rt_polarity\": round(retweet_sentiment, 3),\n",
    "        \"emo_list\": emo_list,\n",
    "        \"pronoun_columns\": printable_pronoun_usage_columns(all_tweet_pc, retweet_pc),\n",
    "    }    \n",
    "    movement_summary = movement_template.safe_substitute(subs)\n",
    "    report_name = f\"{movement}_summary.md\"\n",
    "    report_path = path.join(report_dir, report_name)\n",
    "    with open(report_path, 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(movement_summary)\n",
    "        \n",
    "store_movement_reports(\"Counter\", report_dir, counter_comm_ids, tm)\n",
    "store_movement_reports(\"BLM\", report_dir, blm_comm_ids, tm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the graph\n",
    "graph_file_name = \"graph.pkl\"\n",
    "dir_stem = \"D:/BLM-db/graphs/\" + f\"{period}/\"  \n",
    "graph_file_path = path.join(dir_stem, graph_file_name)\n",
    "tm.urg.g.write_pickle(graph_file_path, version = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_and_previous_activity_template = Template(\"\"\"\n",
    "## Analysis by Previous Activity for Stance $stance\n",
    "\n",
    "Number of previously active accounts: $num_experienced  \n",
    "Number of first-time accounts:        $num_noob\n",
    "\n",
    "### Activity \n",
    "\n",
    "| Activity | No Previous Activity | Previously Active |\n",
    "|------:|:------:|:-------|\n",
    "| Avg Tweets | $noob_tweets | $experienced_tweets |\n",
    "| Avg Retweets | $noob_retweets | $experienced_retweets |\n",
    "| Avg Replies | $noob_replies | $experienced_replies |\n",
    "\n",
    "### Sentiment Analysis\n",
    "\n",
    "| Measure | No Previous Activity | Previously Active |\n",
    "|------:|:------:|:-------|\n",
    "| Avg Sentiment | $noob_sentiment | $experienced_sentiment |\n",
    "$emo_list\n",
    "\n",
    "### Pronoun Usage\n",
    "\n",
    "| Pronoun | No Previous Activity | Previously Active |\n",
    "|------:|:------:|:-------|\n",
    "$pronoun_columns\n",
    "\n",
    "### Top Memes\n",
    "\n",
    "#### No Previous Activity\n",
    "\n",
    "| Count | Hashtag |\n",
    "|------:|:------|\n",
    "$noob_hashtag_list\n",
    "\n",
    "#### Previously Active\n",
    "\n",
    "| Count | Hashtag |\n",
    "|------:|:------|\n",
    "$experienced_hashtag_list\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "global_experienced_accounts = db.get_account_list(end_period = period - 1)\n",
    "global_experienced_accounts = set(global_experienced_accounts)\n",
    "\n",
    "\n",
    "def activity_and_sentiment_for_accounts(accounts: List[str], tm: TweetsManager):\n",
    "    num_accounts = len(accounts)\n",
    "    num_tweets, num_retweets, num_replies = 0, 0, 0\n",
    "    sentiment = 0.0\n",
    "    pronoun_counts = PronounCounts()\n",
    "    emo_scores = EmoScores()\n",
    "    meme_counter = Counter()\n",
    "    if num_accounts > 0:\n",
    "        for account_id in accounts:\n",
    "            ua = tm.user_activity[account_id]\n",
    "            num_tweets += ua.tweet_count\n",
    "            num_retweets += ua.retweet_count\n",
    "            num_replies += ua.reply_count\n",
    "            sentiment += ua.sentiment_summary.sentiment\n",
    "            pronoun_counts += ua.sentiment_summary.pronoun_counts\n",
    "            emo_scores += ua.sentiment_summary.emo_scores\n",
    "            for meme, count in ua.meme_counter.items():\n",
    "                meme_counter[meme] += count\n",
    "        num_tweets /= num_accounts\n",
    "        num_retweets /= num_accounts\n",
    "        num_replies /= num_accounts\n",
    "        sentiment /= num_accounts\n",
    "        pronoun_counts /= num_accounts\n",
    "        emo_scores /= num_accounts\n",
    "    sentiment_analysis = SentimentAnalysis(pronoun_counts, emo_scores, sentiment)\n",
    "    return num_accounts, num_tweets, num_retweets, num_replies, sentiment_analysis, meme_counter    \n",
    "\n",
    "\n",
    "def publish_experience_analysis(stance: str, community_ids: List[int], tm: TweetsManager):\n",
    "    if len(community_ids) == 0:\n",
    "        return\n",
    "    experienced_accounts = []\n",
    "    noob_accounts = []\n",
    "    for community_id in community_ids:\n",
    "        for user_id in tm.community_user_map[community_id]:\n",
    "            if user_id in global_experienced_accounts:\n",
    "                experienced_accounts.append(user_id)\n",
    "            else:\n",
    "                noob_accounts.append(user_id)\n",
    "    num_noob, noob_tweets, noob_retweets, noob_replies, noob_sa, noob_memes = \\\n",
    "        activity_and_sentiment_for_accounts(noob_accounts, tm)\n",
    "    num_exp, exp_tweets, exp_retweets, exp_replies, exp_sa, exp_memes = \\\n",
    "        activity_and_sentiment_for_accounts(experienced_accounts, tm)\n",
    "    subs = {\n",
    "        \"stance\": stance,\n",
    "        \"num_noob\": num_noob,\n",
    "        \"num_experienced\": num_exp,\n",
    "        \"noob_tweets\": round(noob_tweets, 3),\n",
    "        \"experienced_tweets\": round(exp_tweets, 3),\n",
    "        \"noob_retweets\": round(noob_retweets, 3),\n",
    "        \"experienced_retweets\": round(exp_retweets, 3),\n",
    "        \"noob_replies\": round(noob_replies, 3),\n",
    "        \"experienced_replies\": round(exp_replies, 3),\n",
    "        \"noob_sentiment\": round(noob_sa.sentiment, 3),\n",
    "        \"experienced_sentiment\": round(exp_sa.sentiment, 3),\n",
    "        \"emo_list\": printable_emo_scores_columns(left=noob_sa.emo_scores, right=exp_sa.emo_scores),\n",
    "        \"pronoun_columns\": printable_pronoun_usage_columns(left=noob_sa.pronoun_counts, right=exp_sa.pronoun_counts),\n",
    "        \"noob_hashtag_list\": printable_top_hashtag_list(noob_memes),\n",
    "        \"experienced_hashtag_list\": printable_top_hashtag_list(exp_memes),\n",
    "    }\n",
    "    experience_summary = stance_and_previous_activity_template.safe_substitute(subs)\n",
    "    report_name = f\"{stance}_experience_analysis.md\"\n",
    "    report_path = path.join(report_dir, report_name)\n",
    "    with open(report_path, 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(experience_summary)\n",
    "\n",
    "publish_experience_analysis(\"BLM\", blm_comm_ids, tm)\n",
    "publish_experience_analysis(\"CounterProtest\", counter_comm_ids, tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview report\n",
    "overview_template = Template('''\n",
    "## OVERVIEW of PERIOD $start_date to $end_date\n",
    "\n",
    "| What  | How Many |\n",
    "|:-------|--------:|\n",
    "| Tweets | $num_tweets |\n",
    "| Retweets | $num_retweets |  \n",
    "| Communities | $num_communities |  \n",
    "| Accounts | $num_accounts |\n",
    "| Size of largest community | $largest_comm_size |\n",
    "\n",
    "''')\n",
    "total_tweets = sum(ua.tweet_count for ua in tm.user_activity.values())\n",
    "total_retweets = sum(ua.retweet_count for ua in tm.user_activity.values())\n",
    "num_communities = len(tm.community_user_map)\n",
    "num_accounts = len(tm.user_community_map)\n",
    "largest_comm_size = len(comm_user_counts[0][1])\n",
    "\n",
    "subs = {\n",
    "    'start_date': start_date,\n",
    "    'end_date': end_date,\n",
    "    'num_tweets': total_tweets,\n",
    "    'num_retweets': total_retweets,\n",
    "    'num_communities': num_communities,\n",
    "    'num_accounts': num_accounts,\n",
    "    'largest_comm_size': largest_comm_size,\n",
    "}\n",
    "overview_report_name = \"OverviewReport.md\"\n",
    "overview_path = path.join(report_dir, overview_report_name)\n",
    "overview = overview_template.safe_substitute(subs)\n",
    "with open(overview_path, 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(overview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python383jvsc74a57bd0cffc5f2a9d67381facf27223a4588ad4e9006ed836e88cb14d06569e1c04153f",
   "display_name": "Python 3.8.3 64-bit ('blm': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}